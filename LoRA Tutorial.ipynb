{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6b27c-fe63-49fc-9e2f-650e02d7d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this will not run on a Mac M1/M2 \n",
    "# fp16 mixed precision requires a GPU (not 'mps') unless you set fp16=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c887c9d-1f56-427e-8ceb-899f6924e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries.\n",
    "# transformers includes huggingface transformers\n",
    "# datasets for handling data, evaluate for metrics, peft for parameter-efficient\n",
    "# fine-tuning (LoRA), trl for training, and bitsandbytes for quantization.\n",
    "!pip install transformers datasets evaluate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f92eda-ca4c-4a87-b4ff-5e02dd5ec37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # For interacting with the file system\n",
    "import torch  # Core PyTorch library for deep learning\n",
    "from datasets import load_dataset  # For loading and managing datasets\n",
    "from transformers import (  # Hugging Face Transformers library\n",
    "    AutoModelForCausalLM,  # Auto-classes for loading pretrained models\n",
    "    AutoTokenizer,  # Auto-classes for loading pretrained tokenizers\n",
    "    BitsAndBytesConfig,  # For 8-bit quantization of the model\n",
    "    TrainingArguments,  # Configuration for training\n",
    "    pipeline,  # For easier model inference\n",
    "    logging  # For controlling logging output\n",
    ")\n",
    "from peft import LoraConfig  # Configuration for LoRA (Low-Rank Adaptation)\n",
    "from trl import SFTTrainer  # Trainer for Supervised Fine-Tuning (SFT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb06f5a-6ca3-4c9c-a0f2-a37689140c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model and Dataset Setup ---\n",
    "\n",
    "# Specify the base model (a pre-trained Llama model)\n",
    "base_model = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "# Specify the dataset for instruction fine-tuning\n",
    "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "# Specify the name for the new fine-tuned model\n",
    "new_model = \"llama-1.1B-chat-guanaco\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e15386-4477-4b43-8eb5-27d81f8fa740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(guanaco_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91628ed1-dd09-4158-9dd7-8663a0b1cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model, automatically placing layers on available devices\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, device_map='auto')\n",
    "# Disable caching for faster inference but potentially higher memory usage\n",
    "model.config.use_cache = False\n",
    "# Configuration for pre-training tensor parallelism (not relevant for fine-tuning)\n",
    "model.config.pretraining_tp = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c4aca-7582-46ac-92c4-be05843ce9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# Set padding token and padding direction (important for model input)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad89504-7d1e-4613-a4f1-95d18e65aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run a quick inference before fine-tuning ---\n",
    "\n",
    "# Suppress most logging messages for a cleaner output\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Define a sample prompt for testing the model\n",
    "prompt = \"Who is Napoleon Bonaparte?\"\n",
    "# Create a text generation pipeline for easy inference\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "# Run inference and print the result\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22754d6d-1889-46c8-b39b-6f27e7de51af",
   "metadata": {},
   "source": [
    "## --- Fine-Tuning Setup ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f645670-af4d-41f2-ade3-a83afdf285ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA parameters for efficient fine-tuning\n",
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,  # Multiplier for LoRA outputs\n",
    "    lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
    "    r=64,  # Rank (dimensionality) of LoRA matrices\n",
    "    bias=\"none\",  # No bias term in LoRA\n",
    "    task_type=\"CAUSAL_LM\"  # Type of task (Causal Language Modeling)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c5252-8de8-4ce4-bb8a-1ddb431ff547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments for the SFTTrainer\n",
    "training_params = TrainingArguments(\n",
    "    output_dir='./results',  # Output directory for checkpoints and results\n",
    "    num_train_epochs=2,  # Number of training epochs\n",
    "    per_device_train_batch_size=2,  # Batch size per device\n",
    "    gradient_accumulation_steps=16,  # Accumulate gradients over 16 steps\n",
    "    optim=\"adamw_torch\",  # Optimizer (AdamW)\n",
    "    save_steps=25,  # Save a checkpoint every 25 steps\n",
    "    logging_steps=1,  # Log every step\n",
    "    learning_rate=2e-4,  # Learning rate\n",
    "    weight_decay=0.001,  # Weight decay for regularization\n",
    "    fp16=True,  # Use 16-bit precision (if available)\n",
    "    bf16=False,  # Don't use bfloat16 (not supported on all hardware)\n",
    "    max_grad_norm=0.3,  # Gradient clipping for stability\n",
    "    max_steps=-1,  # No limit on the number of training steps\n",
    "    warmup_ratio=0.03,  # Warmup ratio for the learning rate\n",
    "    group_by_length=True,  # Group sequences by length for efficient training\n",
    "    lr_scheduler_type=\"cosine\"  # Cosine learning rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c1ea2-e313-45e8-8609-d95483ea48e5",
   "metadata": {},
   "source": [
    "## --- Fine-Tuning and Saving ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820ed34-2ff4-4ca4-b774-ac88b468c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,  \n",
    "    train_dataset=dataset,  \n",
    "    peft_config=peft_params,  \n",
    "    dataset_text_field=\"text\",  # Text field in the dataset\n",
    "    max_seq_length=None,  # No maximum sequence length\n",
    "    tokenizer=tokenizer,  \n",
    "    args=training_params,  \n",
    "    packing=False  # Don't pack sequences into a single batch \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3e005-3ac2-4326-8c61-329156b2c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually trigger garbage collection and clear GPU cache\n",
    "import gc \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2abfbb-e18d-430f-a81e-e4d0381444c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "trainer.train()\n",
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67733958-9d04-4e3a-9f32-e596b4a4ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Inference After Fine-Tuning ---\n",
    "# Run inference again with the same prompt to see the difference after fine-tuning\n",
    "prompt = \"Who is Napoleon Bonaparte?\"\n",
    "pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f'<s>[INST] {prompt} [/INST]')\n",
    "print(result[0]['generated_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c750c2-4fb6-433d-9e66-04dac67c356b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de6e35-870b-48cd-ba11-821748da7760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2aa4d-548f-4598-9d4a-ed6a5f2c1e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
